{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y00RfkU5gl3e"
      },
      "source": [
        "# pip install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBfYaCjEgokt"
      },
      "outputs": [],
      "source": [
        "pip install gymnasium\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttq6_9qDgp-c"
      },
      "outputs": [],
      "source": [
        "pip install \"stable-baselines3[extra]>=2.0.0a4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6x7QilC85No"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env, SubprocVecEnv, DummyVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlZPhtptcf9v"
      },
      "source": [
        "## set reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfbdcxuNcker"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "os.environ['PYTHONASHSEED'] = '0'\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "\n",
        "seed = 0\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Rate Func"
      ],
      "metadata": {
        "id": "Ly-x3gpq9yWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def linear_schedule(initial_value):\n",
        "\n",
        "    if isinstance(initial_value, str):\n",
        "        initial_value = float(initial_value)\n",
        "\n",
        "    def func(progress):\n",
        "\n",
        "        return progress * initial_value\n",
        "\n",
        "    return func"
      ],
      "metadata": {
        "id": "G1WxvxqS901I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEsAU3iFchK7"
      },
      "source": [
        "# Ranking Func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ_VXtt5dYfZ"
      },
      "outputs": [],
      "source": [
        "#local modules directory for import\n",
        "DIR = '/xxhome/' # replace with utils home directory\n",
        "\n",
        "\n",
        "sys.path.append(DIR)\n",
        "\n",
        "\n",
        "# import utils fns\n",
        "from utils.read_data_fns import *\n",
        "from utils.eval_fns import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_targets = pd.DataFrame()\n",
        "DRL_DIR = '/xx/' # replace with working directory\n"
      ],
      "metadata": {
        "id": "iV2VUWqzvrIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjCDeTJ5UKiW"
      },
      "source": [
        "# TAREnv class\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SELECTED_TOPICS = [] # keep track of all randomly selected topics\n"
      ],
      "metadata": {
        "id": "RpIO2YndUKiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import RL env\n",
        "from rl_utils.rlstop_tar_env import *\n",
        "from rl_utils.ranking_utils import *\n",
        "\n"
      ],
      "metadata": {
        "id": "2zQLSs42xEgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5NLBfXJUKic"
      },
      "source": [
        "## Hyperparameter Settings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING = True\n",
        "total_runs = 10\n",
        "\n",
        "# Train the agent\n",
        "ent_coef = 0.01\n",
        "\n",
        "gamma = 0.99\n",
        "learning_rate_initial = 0.0001\n",
        "\n",
        "learning_rate = linear_schedule(learning_rate_initial)\n",
        "clip_range=0.2\n",
        "\n",
        "n_steps = 100\n",
        "batch_size = 100\n",
        "n_epochs =8\n",
        "\n",
        "model_name = 'reward_1-1_'\n",
        "learning_rate_type = '_linear_schedule'+str(learning_rate_initial)\n",
        "\n",
        "learning_rate_type = '_lr_static'+str(learning_rate_initial)\n",
        "\n",
        "total_timesteps = 100_000\n",
        "\n",
        "tensorboard_log = '/logs/'\n",
        "\n"
      ],
      "metadata": {
        "id": "OWMc64-9UKic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Target 0.9"
      ],
      "metadata": {
        "id": "bb5t0nnWvrIw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trwC5TvOvrIx"
      },
      "outputs": [],
      "source": [
        "target_recall = 0.9 # replace with other target recall level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9021EE_uvrIx"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIYgrwPsvrIx"
      },
      "outputs": [],
      "source": [
        "TRAINING = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = 'CLEF'\n"
      ],
      "metadata": {
        "id": "Xq06AKLsmzai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQNtQe-CvrIy"
      },
      "source": [
        "#### sort topics by target location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abjOyIwavrIy"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset_name = 'CLEF2017'\n",
        "\n",
        "\n",
        "qrels = \"data/qrels/CLEF2017_qrels.txt\"\n",
        "\n",
        "\n",
        "qrel_fname, query_rel_dic = load_rel_data(qrels)\n",
        "print(\"Number of topics:\", len(query_rel_dic))\n",
        "\n",
        "run = \"data/rankings/clef2017_training_ranking.txt\"\n",
        "\n",
        "doc_rank_dic, rank_rel_dic = load_run_data(run)\n",
        "\n",
        "topics_list = make_topics_list(doc_rank_dic,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0GYWsXNvrIy"
      },
      "outputs": [],
      "source": [
        "#remove topic CD008760 last element, contains 64 items only, < 100 vector size\n",
        "topics_list= topics_list[:-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPXvdDZyvrIz"
      },
      "outputs": [],
      "source": [
        "topics_info = []\n",
        "\n",
        "for t in topics_list:\n",
        "  topic_id, n_docs, n_rel, prev, target_location = load_topic_target_location(t,target_recall)\n",
        "  print(topic_id, n_docs, n_rel, round(prev,3), target_location)\n",
        "  topics_info.append([topic_id, n_docs, n_rel, prev, target_location])\n",
        "\n",
        "topics_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUOvCF3ZvrIz"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.DataFrame(topics_info, columns=['topic_id', 'n_docs', 'n_rel', 'prev', 'target_location'])\n",
        "df = df.sort_values(by=['target_location'])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AMp4j8MvrIz"
      },
      "outputs": [],
      "source": [
        "sorted_target_loc_topics = list(df['topic_id'])\n",
        "sorted_target_loc_topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2YVvKbNvrI0"
      },
      "source": [
        "####ordered topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGT5clWFvrI0"
      },
      "outputs": [],
      "source": [
        "TRAINING = True\n",
        "\n",
        "SELECTED_TOPICS_ORDERERD = sorted_target_loc_topics\n",
        "SELECTED_TOPICS_ORDERERD_INDEX = 0\n",
        "\n",
        "# Instantiate the vec env\n",
        "\n",
        "#random topic selection for each env instance\n",
        "SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n",
        "\n",
        "vec_env = make_vec_env(TAREnv, n_envs=len(topics_list), env_kwargs=dict(target_recall=target_recall, topics_list = topics_list, topic_id=None, size=100, render_mode='human'))\n",
        "\n",
        "SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n",
        "\n",
        "train_size = len(topics_list)\n",
        "vec_env_train = vec_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwRZ2csfvrI0"
      },
      "source": [
        "#### PPO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "KFn_jJTMvrI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tb_log_name = model_name+\"_\"+training_dataset+\"_ppo_gma_\"+str(gamma)+\"_nsteps\"+str(n_steps)+\"_btch\"+str(batch_size)+\"_timesteps_\"+str(total_timesteps)+ \"_ent_coef\"+str(ent_coef)+ learning_rate_type +\"_n_epochs\"+str(n_epochs)+\"_target\"+str(target_recall)\n",
        "\n",
        "model = PPO(\n",
        "    policy = 'MlpPolicy',\n",
        "    env = vec_env_train,\n",
        "    n_steps = n_steps,\n",
        "    batch_size = batch_size,\n",
        "    n_epochs = n_epochs,\n",
        "    gamma = gamma,\n",
        "    gae_lambda = 0.98,\n",
        "    ent_coef = ent_coef,\n",
        "    verbose=1,\n",
        "    learning_rate = learning_rate,\n",
        "    seed=0,\n",
        "    tensorboard_log= tensorboard_log)\n",
        "\n",
        "model.learn(total_timesteps=total_timesteps, tb_log_name=tb_log_name)\n",
        "\n",
        "\n",
        "model.save(tensorboard_log+'model_'+tb_log_name)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TmvIXFQLvrI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir \"$tensorboard_log\"\n"
      ],
      "metadata": {
        "id": "uAH0YW8zvrI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0K6vm3EvrI5"
      },
      "source": [
        "## TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD8taXFIvrI5"
      },
      "outputs": [],
      "source": [
        "TRAINING = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dafJeCGvrI5"
      },
      "source": [
        "###clef2017"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISshquTgvrI6"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset_name = 'CLEF2017'\n",
        "\n",
        "\n",
        "qrels = \"data/qrels/CLEF2017_qrels.txt\"\n",
        "\n",
        "\n",
        "qrel_fname, query_rel_dic = load_rel_data(qrels)\n",
        "print(\"Number of topics:\", len(query_rel_dic))\n",
        "\n",
        "run = \"data/rankings/clef2017_ranking.txt\"\n",
        "\n",
        "doc_rank_dic, rank_rel_dic = load_run_data(run)\n",
        "\n",
        "topics_list = make_topics_list(doc_rank_dic,1)  # sort topics by no docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KexHxgvvrI6"
      },
      "outputs": [],
      "source": [
        "# Instantiate the vec env\n",
        "\n",
        "#random topic selection for each env instance\n",
        "SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n",
        "\n",
        "vec_env = make_vec_env(TAREnv, n_envs=len(topics_list), env_kwargs=dict(target_recall=target_recall, topics_list = topics_list, topic_id=None, size=100, render_mode='human'))\n",
        "\n",
        "SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtkW9xzQvrI6"
      },
      "outputs": [],
      "source": [
        "test_size = len(topics_list)\n",
        "vec_env_test = vec_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhBug1u_wGMs"
      },
      "source": [
        "#### TAR Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQz44x48wGM1"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.DataFrame()\n",
        "df_all_runs = pd.DataFrame()\n",
        "\n",
        "for run in range(total_runs):\n",
        "\n",
        "  # Test the trained agent\n",
        "  vec_env_test = vec_env\n",
        "  obs = vec_env_test.reset()\n",
        "  test_steps = 100\n",
        "\n",
        "\n",
        "  n_env = test_size\n",
        "  agent=0\n",
        "  target=0\n",
        "  agent_vector=[]\n",
        "  terminal_observation=[]\n",
        "\n",
        "  topics = []\n",
        "  recalls = []\n",
        "  costs=[]\n",
        "  e_costs = []\n",
        "  reliabilities = []\n",
        "  rewards = []\n",
        "  distances = []\n",
        "  differences = []\n",
        "  targets = []\n",
        "  run_cnts = []\n",
        "\n",
        "  for eID in range(test_size):\n",
        "\n",
        "    env = vec_env_test.envs[eID]\n",
        "    obs, info = env.reset()\n",
        "\n",
        "    for step in range(test_steps):\n",
        "      action, _ = model.predict(obs, deterministic=False) # predict all next steps\n",
        "      obs, reward, done, trun,info = env.step(action)\n",
        "\n",
        "\n",
        "      if done or trun:\n",
        "                  topic_id = info['topic_id']\n",
        "                  recall = info['recall']\n",
        "                  cost = info['cost']\n",
        "                  e_cost =  ((info['agent'] - info['target']) / (100-info['target']))\n",
        "                  distance = info['distance']\n",
        "\n",
        "                  agent = info['agent']\n",
        "                  target = info['target']\n",
        "                  agent_vector = info['agent_vector']\n",
        "                  terminal_observation = info['terminal_observation']\n",
        "\n",
        "                  difference = target_recall - recall\n",
        "\n",
        "                  reliability = 1 if recall >= target_recall else 0\n",
        "                  topics.append(topic_id)\n",
        "                  recalls.append(recall)\n",
        "                  costs.append(cost)\n",
        "                  e_costs.append(e_cost)\n",
        "                  reliabilities.append(reliability)\n",
        "                  rewards.append(reward)\n",
        "                  distances.append(distance)\n",
        "                  targets.append(target)\n",
        "                  run_cnts.append(run)\n",
        "                  differences.append(difference)\n",
        "\n",
        "                  df_tmp = pd.DataFrame( list(zip([dataset_name]*len(topics_list), topics, run_cnts, recalls, reliabilities, costs, e_costs, rewards, differences, distances, targets)),\n",
        "                  columns =['Dataset', 'Topic', 'Run', 'Recall', 'Reliability', 'Cost', 'e-Cost', 'Reward', 'Difference', 'Distance', 'Target'])\n",
        "\n",
        "                  df = pd.concat([df_tmp])\n",
        "\n",
        "                  df.groupby('Topic').mean()\n",
        "\n",
        "                  break\n",
        "\n",
        "  display(df)\n",
        "  df.groupby('Topic').mean()\n",
        "  df_all_runs = pd.concat([df_all_runs, df])\n",
        "\n",
        "df_all_runs['Model'] = model_name\n",
        "df_all_runs['Model_settings'] = tb_log_name\n",
        "df_all_runs['Target_Recall'] = target_recall\n",
        "df_all_runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SnVP48swGM1"
      },
      "outputs": [],
      "source": [
        "display(df_all_runs.groupby('Topic').mean())\n",
        "display(df_all_runs.groupby('Topic').std())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gubag6SqwGM1"
      },
      "source": [
        "#### df_all_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZbmZaUSwGM2"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_all_targets = pd.concat([df_all_targets, df_all_runs], ignore_index = True)\n",
        "\n",
        "\n",
        "display(df_all_targets)\n",
        "\n",
        "df_all_targets.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_all_targets.groupby(['Target_Recall','Dataset']).mean().round(3))\n",
        "display(df_all_targets.groupby(['Target_Recall','Dataset']).std().round(3))"
      ],
      "metadata": {
        "id": "obDrqj6ywGM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0qE1pq7vrI_"
      },
      "source": [
        "###clef2018"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjWOuwE6vrJA"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset_name = 'CLEF2018'\n",
        "\n",
        "\n",
        "qrels = \"data/qrels/CLEF2018_qrels_LiKs.txt\" # use the same qrel list as their rankings\n",
        "\n",
        "\n",
        "qrel_fname, query_rel_dic = load_rel_data(qrels)\n",
        "print(\"Number of topics:\", len(query_rel_dic))\n",
        "\n",
        "run = \"data/rankings/clef2018_ranking.txt\"\n",
        "\n",
        "doc_rank_dic, rank_rel_dic = load_run_data(run)\n",
        "\n",
        "topics_list = make_topics_list(doc_rank_dic,1)  # sort topics by no docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpxW7H4MvrJB"
      },
      "outputs": [],
      "source": [
        "# Instantiate the vec env\n",
        "\n",
        "#random topic selection for each env instance\n",
        "SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n",
        "\n",
        "vec_env = make_vec_env(TAREnv, n_envs=len(topics_list), env_kwargs=dict(target_recall=target_recall, topics_list = topics_list, topic_id=None, size=100, render_mode='human'))\n",
        "\n",
        "SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MosHTrphvrJB"
      },
      "outputs": [],
      "source": [
        "test_size = len(topics_list)\n",
        "vec_env_test = vec_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuTDj0W2wWye"
      },
      "source": [
        "#### TAR Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v_QYNjxwWyf"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.DataFrame()\n",
        "df_all_runs = pd.DataFrame()\n",
        "\n",
        "for run in range(total_runs):\n",
        "\n",
        "  # Test the trained agent\n",
        "  vec_env_test = vec_env\n",
        "  obs = vec_env_test.reset()\n",
        "  test_steps = 100\n",
        "\n",
        "\n",
        "  n_env = test_size\n",
        "  agent=0\n",
        "  target=0\n",
        "  agent_vector=[]\n",
        "  terminal_observation=[]\n",
        "\n",
        "  topics = []\n",
        "  recalls = []\n",
        "  costs=[]\n",
        "  e_costs = []\n",
        "  reliabilities = []\n",
        "  rewards = []\n",
        "  distances = []\n",
        "  differences = []\n",
        "  targets = []\n",
        "  run_cnts = []\n",
        "\n",
        "  for eID in range(test_size):\n",
        "\n",
        "    env = vec_env_test.envs[eID]\n",
        "    obs, info = env.reset()\n",
        "\n",
        "    for step in range(test_steps):\n",
        "      action, _ = model.predict(obs, deterministic=False) # predict all next steps\n",
        "      obs, reward, done, trun,info = env.step(action)\n",
        "\n",
        "\n",
        "      if done or trun:\n",
        "                  topic_id = info['topic_id']\n",
        "                  recall = info['recall']\n",
        "                  cost = info['cost']\n",
        "                  e_cost =  ((info['agent'] - info['target']) / (100-info['target']))\n",
        "                  distance = info['distance']\n",
        "\n",
        "                  agent = info['agent']\n",
        "                  target = info['target']\n",
        "                  agent_vector = info['agent_vector']\n",
        "                  terminal_observation = info['terminal_observation']\n",
        "\n",
        "                  difference = target_recall - recall\n",
        "\n",
        "                  reliability = 1 if recall >= target_recall else 0\n",
        "                  topics.append(topic_id)\n",
        "                  recalls.append(recall)\n",
        "                  costs.append(cost)\n",
        "                  e_costs.append(e_cost)\n",
        "                  reliabilities.append(reliability)\n",
        "                  rewards.append(reward)\n",
        "                  distances.append(distance)\n",
        "                  targets.append(target)\n",
        "                  run_cnts.append(run)\n",
        "                  differences.append(difference)\n",
        "\n",
        "                  df_tmp = pd.DataFrame( list(zip([dataset_name]*len(topics_list), topics, run_cnts, recalls, reliabilities, costs, e_costs, rewards, differences, distances, targets)),\n",
        "                  columns =['Dataset', 'Topic', 'Run', 'Recall', 'Reliability', 'Cost', 'e-Cost', 'Reward', 'Difference', 'Distance', 'Target'])\n",
        "\n",
        "                  df = pd.concat([df_tmp])\n",
        "\n",
        "                  df.groupby('Topic').mean()\n",
        "\n",
        "                  break\n",
        "\n",
        "  display(df)\n",
        "  df.groupby('Topic').mean()\n",
        "  df_all_runs = pd.concat([df_all_runs, df])\n",
        "\n",
        "df_all_runs['Model'] = model_name\n",
        "df_all_runs['Model_settings'] = tb_log_name\n",
        "df_all_runs['Target_Recall'] = target_recall\n",
        "df_all_runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAbMOIr5wWyf"
      },
      "outputs": [],
      "source": [
        "display(df_all_runs.groupby('Topic').mean())\n",
        "display(df_all_runs.groupby('Topic').std())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSe33FehwWyg"
      },
      "source": [
        "#### df_all_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brGT0e9CwWyg"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_all_targets = pd.concat([df_all_targets, df_all_runs], ignore_index = True)\n",
        "\n",
        "\n",
        "display(df_all_targets)\n",
        "\n",
        "df_all_targets.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_all_targets.groupby(['Target_Recall','Dataset']).mean().round(3))\n",
        "display(df_all_targets.groupby(['Target_Recall','Dataset']).std().round(3))"
      ],
      "metadata": {
        "id": "HnTgvIYcwWyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVrxyUwMvrJF"
      },
      "source": [
        "###clef2019"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8oyykHyvrJG"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset_name = 'CLEF2019'\n",
        "\n",
        "\n",
        "qrels = \"data/qrels/CLEF2019_qrels_LiKs.txt\" # use the same qrel list as their rankings\n",
        "\n",
        "\n",
        "qrel_fname, query_rel_dic = load_rel_data(qrels)\n",
        "print(\"Number of topics:\", len(query_rel_dic))\n",
        "\n",
        "run = \"data/rankings/clef2019_ranking.txt\"\n",
        "\n",
        "doc_rank_dic, rank_rel_dic = load_run_data(run)\n",
        "\n",
        "topics_list = make_topics_list(doc_rank_dic,1)  # sort topics by no docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAXfzS79vrJG"
      },
      "outputs": [],
      "source": [
        "#remove topic CD012164 last element, contains 61 items only, < 100 vector size\n",
        "topics_list= topics_list[:-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAdCPfZvvrJG"
      },
      "outputs": [],
      "source": [
        "# Instantiate the vec env\n",
        "\n",
        "#random topic selection for each env instance\n",
        "SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n",
        "\n",
        "vec_env = make_vec_env(TAREnv, n_envs=len(topics_list), env_kwargs=dict(target_recall=target_recall, topics_list = topics_list, topic_id=None, size=100, render_mode='human'))\n",
        "\n",
        "SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W3TCs-WvrJH"
      },
      "outputs": [],
      "source": [
        "test_size = len(topics_list)\n",
        "vec_env_test = vec_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQTp13XvwihX"
      },
      "source": [
        "#### TAR Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPyDpeudwihi"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.DataFrame()\n",
        "df_all_runs = pd.DataFrame()\n",
        "\n",
        "for run in range(total_runs):\n",
        "\n",
        "  # Test the trained agent\n",
        "  vec_env_test = vec_env\n",
        "  obs = vec_env_test.reset()\n",
        "  test_steps = 100\n",
        "\n",
        "\n",
        "  n_env = test_size\n",
        "  agent=0\n",
        "  target=0\n",
        "  agent_vector=[]\n",
        "  terminal_observation=[]\n",
        "\n",
        "  topics = []\n",
        "  recalls = []\n",
        "  costs=[]\n",
        "  e_costs = []\n",
        "  reliabilities = []\n",
        "  rewards = []\n",
        "  distances = []\n",
        "  differences = []\n",
        "  targets = []\n",
        "  run_cnts = []\n",
        "\n",
        "  for eID in range(test_size):\n",
        "\n",
        "    env = vec_env_test.envs[eID]\n",
        "    obs, info = env.reset()\n",
        "\n",
        "    for step in range(test_steps):\n",
        "      action, _ = model.predict(obs, deterministic=False) # predict all next steps\n",
        "      obs, reward, done, trun,info = env.step(action)\n",
        "\n",
        "\n",
        "      if done or trun:\n",
        "                  topic_id = info['topic_id']\n",
        "                  recall = info['recall']\n",
        "                  cost = info['cost']\n",
        "                  e_cost =  ((info['agent'] - info['target']) / (100-info['target']))\n",
        "                  distance = info['distance']\n",
        "\n",
        "                  agent = info['agent']\n",
        "                  target = info['target']\n",
        "                  agent_vector = info['agent_vector']\n",
        "                  terminal_observation = info['terminal_observation']\n",
        "\n",
        "                  difference = target_recall - recall\n",
        "\n",
        "                  reliability = 1 if recall >= target_recall else 0\n",
        "                  topics.append(topic_id)\n",
        "                  recalls.append(recall)\n",
        "                  costs.append(cost)\n",
        "                  e_costs.append(e_cost)\n",
        "                  reliabilities.append(reliability)\n",
        "                  rewards.append(reward)\n",
        "                  distances.append(distance)\n",
        "                  targets.append(target)\n",
        "                  run_cnts.append(run)\n",
        "                  differences.append(difference)\n",
        "\n",
        "                  df_tmp = pd.DataFrame( list(zip([dataset_name]*len(topics_list), topics, run_cnts, recalls, reliabilities, costs, e_costs, rewards, differences, distances, targets)),\n",
        "                  columns =['Dataset', 'Topic', 'Run', 'Recall', 'Reliability', 'Cost', 'e-Cost', 'Reward', 'Difference', 'Distance', 'Target'])\n",
        "\n",
        "                  df = pd.concat([df_tmp])\n",
        "\n",
        "                  df.groupby('Topic').mean()\n",
        "\n",
        "                  break\n",
        "\n",
        "  display(df)\n",
        "  df.groupby('Topic').mean()\n",
        "  df_all_runs = pd.concat([df_all_runs, df])\n",
        "\n",
        "df_all_runs['Model'] = model_name\n",
        "df_all_runs['Model_settings'] = tb_log_name\n",
        "df_all_runs['Target_Recall'] = target_recall\n",
        "df_all_runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjJBr2GDwihi"
      },
      "outputs": [],
      "source": [
        "display(df_all_runs.groupby('Topic').mean())\n",
        "display(df_all_runs.groupby('Topic').std())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9YTjSUzwihi"
      },
      "source": [
        "#### df_all_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr2MSvOkwihi"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_all_targets = pd.concat([df_all_targets, df_all_runs], ignore_index = True)\n",
        "\n",
        "\n",
        "display(df_all_targets)\n",
        "\n",
        "df_all_targets.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_all_targets.groupby(['Target_Recall','Dataset']).mean().round(3))\n",
        "display(df_all_targets.groupby(['Target_Recall','Dataset']).std().round(3))"
      ],
      "metadata": {
        "id": "J6L4WsoZwihj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okU3R-cavrJX"
      },
      "source": [
        "##TREC-TR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdq03417vrJX"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Y6FxglsvrJY"
      },
      "outputs": [],
      "source": [
        "TRAINING = True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = 'TREC-TR'"
      ],
      "metadata": {
        "id": "dTJXJnoLm7I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O9mQRL1vrJY"
      },
      "source": [
        "#### sort topics by target location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmCjOYmPvrJY"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "dataset_name = 'TREC-TR'\n",
        "\n",
        "qrels = \"data/qrels/TREC_TR_Training_qrels.txt\"\n",
        "\n",
        "\n",
        "qrel_fname, query_rel_dic = load_rel_data(qrels)\n",
        "print(\"Number of topics:\", len(query_rel_dic))\n",
        "\n",
        "run = \"data/rankings/tr_training_ranking.txt\"\n",
        "\n",
        "doc_rank_dic, rank_rel_dic = load_run_data(run)\n",
        "\n",
        "\n",
        "\n",
        "topics_list = make_topics_list(doc_rank_dic,1)  # sort topics by no docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UDM_l2LvrJY"
      },
      "outputs": [],
      "source": [
        "topics_info = []\n",
        "\n",
        "for t in topics_list:\n",
        "  topic_id, n_docs, n_rel, prev, target_location = load_topic_target_location(t,target_recall)\n",
        "  print(topic_id, n_docs, n_rel, round(prev,3), target_location)\n",
        "  topics_info.append([topic_id, n_docs, n_rel, prev, target_location])\n",
        "\n",
        "topics_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iffEghQyvrJZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.DataFrame(topics_info, columns=['topic_id', 'n_docs', 'n_rel', 'prev', 'target_location'])\n",
        "df = df.sort_values(by=['target_location'])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkDuPxr9vrJZ"
      },
      "outputs": [],
      "source": [
        "sorted_target_loc_topics = list(df['topic_id'])\n",
        "sorted_target_loc_topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQB57TljvrJZ"
      },
      "source": [
        "####ordered topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWX8tKV-vrJa"
      },
      "outputs": [],
      "source": [
        "TRAINING = True\n",
        "\n",
        "SELECTED_TOPICS_ORDERERD = sorted_target_loc_topics\n",
        "SELECTED_TOPICS_ORDERERD_INDEX = 0\n",
        "# Instantiate the vec env\n",
        "\n",
        "#random topic selection for each env instance\n",
        "SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n",
        "\n",
        "vec_env = make_vec_env(TAREnv, n_envs=len(topics_list), env_kwargs=dict(target_recall=target_recall, topics_list = topics_list, topic_id=None, size=100, render_mode='human'))\n",
        "\n",
        "SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n",
        "\n",
        "train_size = len(topics_list)\n",
        "vec_env_train = vec_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbFV4gU7vrJa"
      },
      "source": [
        "####  PPO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tb_log_name = model_name+\"_\"+training_dataset+\"_ppo_gma_\"+str(gamma)+\"_nsteps\"+str(n_steps)+\"_btch\"+str(batch_size)+\"_timesteps_\"+str(total_timesteps)+ \"_ent_coef\"+str(ent_coef)+ learning_rate_type +\"_n_epochs\"+str(n_epochs)+\"_target\"+str(target_recall)\n",
        "\n",
        "model = PPO(\n",
        "    policy = 'MlpPolicy',\n",
        "    env = vec_env_train,\n",
        "    n_steps = n_steps,\n",
        "    batch_size = batch_size,\n",
        "    n_epochs = n_epochs,\n",
        "    gamma = gamma,\n",
        "    gae_lambda = 0.98,\n",
        "    ent_coef = ent_coef,\n",
        "    verbose=1,\n",
        "    learning_rate = learning_rate,\n",
        "    seed=0,\n",
        "    tensorboard_log= tensorboard_log)\n",
        "\n",
        "model.learn(total_timesteps=total_timesteps, tb_log_name=tb_log_name)\n",
        "\n",
        "\n",
        "model.save(tensorboard_log+'model_'+tb_log_name)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VMzb_RSbvrJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir \"$tensorboard_log\"\n"
      ],
      "metadata": {
        "id": "-97Qxzw6vrJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "6POEsMmrvrJd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ld0qV5DvrJd"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'TREC-TR'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "qrels = \"data/qrels/TREC_TR_Test_qrels.txt\" # use the same qrel list as their rankings\n",
        "\n",
        "\n",
        "qrel_fname, query_rel_dic = load_rel_data(qrels)\n",
        "print(\"Number of topics:\", len(query_rel_dic))\n",
        "\n",
        "run = \"data/rankings/tr_test_ranking.txt\"\n",
        "\n",
        "doc_rank_dic, rank_rel_dic = load_run_data(run)\n",
        "\n",
        "topics_list = make_topics_list(doc_rank_dic,1)  # sort topics by no docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQiLKjw4vrJd"
      },
      "outputs": [],
      "source": [
        "# Instantiate the vec env\n",
        "\n",
        "#random topic selection for each env instance\n",
        "SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n",
        "\n",
        "vec_env = make_vec_env(TAREnv, n_envs=len(topics_list), env_kwargs=dict(target_recall=target_recall, topics_list = topics_list, topic_id=None, size=100, render_mode='human'))\n",
        "\n",
        "SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvq16rWzvrJe"
      },
      "outputs": [],
      "source": [
        "test_size = len(topics_list)\n",
        "vec_env_test = vec_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5HXnpXXxHsh"
      },
      "source": [
        "#### TAR Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXHH6-RuxHsw"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.DataFrame()\n",
        "df_all_runs = pd.DataFrame()\n",
        "\n",
        "for run in range(total_runs):\n",
        "\n",
        "  # Test the trained agent\n",
        "  vec_env_test = vec_env\n",
        "  obs = vec_env_test.reset()\n",
        "  test_steps = 100\n",
        "\n",
        "\n",
        "  n_env = test_size\n",
        "  agent=0\n",
        "  target=0\n",
        "  agent_vector=[]\n",
        "  terminal_observation=[]\n",
        "\n",
        "  topics = []\n",
        "  recalls = []\n",
        "  costs=[]\n",
        "  e_costs = []\n",
        "  reliabilities = []\n",
        "  rewards = []\n",
        "  distances = []\n",
        "  differences = []\n",
        "  targets = []\n",
        "  run_cnts = []\n",
        "\n",
        "  for eID in range(test_size):\n",
        "\n",
        "    env = vec_env_test.envs[eID]\n",
        "    obs, info = env.reset()\n",
        "\n",
        "    for step in range(test_steps):\n",
        "      action, _ = model.predict(obs, deterministic=False) # predict all next steps\n",
        "      obs, reward, done, trun,info = env.step(action)\n",
        "\n",
        "\n",
        "      if done or trun:\n",
        "                  topic_id = info['topic_id']\n",
        "                  recall = info['recall']\n",
        "                  cost = info['cost']\n",
        "                  e_cost =  ((info['agent'] - info['target']) / (100-info['target']))\n",
        "                  distance = info['distance']\n",
        "\n",
        "                  agent = info['agent']\n",
        "                  target = info['target']\n",
        "                  agent_vector = info['agent_vector']\n",
        "                  terminal_observation = info['terminal_observation']\n",
        "\n",
        "                  difference = target_recall - recall\n",
        "\n",
        "                  reliability = 1 if recall >= target_recall else 0\n",
        "                  topics.append(topic_id)\n",
        "                  recalls.append(recall)\n",
        "                  costs.append(cost)\n",
        "                  e_costs.append(e_cost)\n",
        "                  reliabilities.append(reliability)\n",
        "                  rewards.append(reward)\n",
        "                  distances.append(distance)\n",
        "                  targets.append(target)\n",
        "                  run_cnts.append(run)\n",
        "                  differences.append(difference)\n",
        "\n",
        "                  df_tmp = pd.DataFrame( list(zip([dataset_name]*len(topics_list), topics, run_cnts, recalls, reliabilities, costs, e_costs, rewards, differences, distances, targets)),\n",
        "                  columns =['Dataset', 'Topic', 'Run', 'Recall', 'Reliability', 'Cost', 'e-Cost', 'Reward', 'Difference', 'Distance', 'Target'])\n",
        "\n",
        "                  df = pd.concat([df_tmp])\n",
        "\n",
        "                  df.groupby('Topic').mean()\n",
        "\n",
        "                  break\n",
        "\n",
        "  display(df)\n",
        "  df.groupby('Topic').mean()\n",
        "  df_all_runs = pd.concat([df_all_runs, df])\n",
        "\n",
        "df_all_runs['Model'] = model_name\n",
        "df_all_runs['Model_settings'] = tb_log_name\n",
        "df_all_runs['Target_Recall'] = target_recall\n",
        "df_all_runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LG6WOQPxHsw"
      },
      "outputs": [],
      "source": [
        "display(df_all_runs.groupby('Topic').mean())\n",
        "display(df_all_runs.groupby('Topic').std())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDrsd0SsxHsx"
      },
      "source": [
        "#### df_all_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p4LDx6wxHsx"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_all_targets = pd.concat([df_all_targets, df_all_runs], ignore_index = True)\n",
        "\n",
        "\n",
        "display(df_all_targets)\n",
        "\n",
        "df_all_targets.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_all_targets.groupby(['Target_Recall','Dataset']).mean().round(3))\n",
        "display(df_all_targets.groupby(['Target_Recall','Dataset']).std().round(3))"
      ],
      "metadata": {
        "id": "DGDWqN3HxHsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15Z-d0Wf94UE"
      },
      "source": [
        "##RCV1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MUDYu6s94UN"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANsBkV9094UR"
      },
      "outputs": [],
      "source": [
        "TRAINING = True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = 'RCV1'"
      ],
      "metadata": {
        "id": "C1shTQoJnF9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQtwuarE94UO"
      },
      "source": [
        "#### sort topics by target location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-UtBjXC94US"
      },
      "outputs": [],
      "source": [
        "# LOAD RUN DATA\n",
        "def load_run_data(run):\n",
        "  run_fname = os.path.join(DIR, run)\n",
        "  with open(run_fname, 'r', encoding='utf-8-sig') as infile: # resolve file encoding problem !!\n",
        "    run_data = infile.readlines()\n",
        "  doc_rank_dic = make_rank_dic(run_data)  # make dictionary of ranked docids for each queryid\n",
        "  rank_rel_dic = make_rank_rel_dic(query_rel_dic,doc_rank_dic) # make dic of list relevances of ranked docs for each queryid\n",
        "\n",
        "  #return doc_rank_dic, rank_rel_dic, rank_text_dic\n",
        "  return doc_rank_dic, rank_rel_dic\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "dataset_name = 'RCV1'\n",
        "\n",
        "qrels = \"data/qrels/rcv1_qrels_selected_wo45_0.2.txt\" # use the same qrel list as their rankings\n",
        "\n",
        "\n",
        "qrel_fname, query_rel_dic = load_rel_data(qrels)\n",
        "print(\"Number of topics:\", len(query_rel_dic))\n",
        "\n",
        "run = \"data/rankings/RCV1_test_20wo45_ranking.txt\"\n",
        "run = \"data/rankings/temp/RCV1_test_20wo45_ranking_utf8_2.txt\"\n",
        "\n",
        "doc_rank_dic, rank_rel_dic = load_run_data(run)###\n",
        "\n",
        "\n",
        "\n",
        "topics_list = make_topics_list(doc_rank_dic,1)  # sort topics by no docs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# only selected 1st level topics\n",
        "topics_list = ['C151', 'C171', 'C181', 'C311', 'C331', 'C411', 'E121', 'E131', 'E141', 'E211', 'E311', 'E411', 'E511', 'G151', 'M131', 'M141']"
      ],
      "metadata": {
        "id": "XAaFlDz-SvfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx5HB76v94US"
      },
      "outputs": [],
      "source": [
        "topics_info = []\n",
        "\n",
        "for t in topics_list:\n",
        "  topic_id, n_docs, n_rel, prev, target_location = load_topic_target_location(t,target_recall)\n",
        "  print(topic_id, n_docs, n_rel, round(prev,3), target_location)\n",
        "  topics_info.append([topic_id, n_docs, n_rel, prev, target_location])\n",
        "\n",
        "topics_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSt6_4P194US"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.DataFrame(topics_info, columns=['topic_id', 'n_docs', 'n_rel', 'prev', 'target_location'])\n",
        "df = df.sort_values(by=['target_location'])\n",
        "#df = df.sort_values(by=['target_location'],ascending=False)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H6dEaeY94US"
      },
      "outputs": [],
      "source": [
        "sorted_target_loc_topics = list(df['topic_id'])\n",
        "sorted_target_loc_topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAwxbZqu94US"
      },
      "source": [
        "####ordered topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyK4_aDC94US"
      },
      "outputs": [],
      "source": [
        "TRAINING = True\n",
        "\n",
        "SELECTED_TOPICS_ORDERERD = sorted_target_loc_topics\n",
        "SELECTED_TOPICS_ORDERERD_INDEX = 0\n",
        "# Instantiate the vec env\n",
        "\n",
        "#random topic selection for each env instance\n",
        "SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n",
        "\n",
        "vec_env = make_vec_env(TAREnv, n_envs=len(topics_list), env_kwargs=dict(target_recall=target_recall, topics_list = topics_list, topic_id=None, size=100, render_mode='human'))\n",
        "\n",
        "SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n",
        "\n",
        "train_size = len(topics_list)\n",
        "vec_env_train = vec_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC2NS_xl94UT"
      },
      "source": [
        "#### PPO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tb_log_name = model_name+\"_\"+training_dataset+\"_ppo_gma_\"+str(gamma)+\"_nsteps\"+str(n_steps)+\"_btch\"+str(batch_size)+\"_timesteps_\"+str(total_timesteps)+ \"_ent_coef\"+str(ent_coef)+ learning_rate_type +\"_n_epochs\"+str(n_epochs)+\"_target\"+str(target_recall)\n",
        "\n",
        "model = PPO(\n",
        "    policy = 'MlpPolicy',\n",
        "    env = vec_env_train,\n",
        "    n_steps = n_steps,\n",
        "    batch_size = batch_size,\n",
        "    n_epochs = n_epochs,\n",
        "    gamma = gamma,\n",
        "    gae_lambda = 0.98,\n",
        "    ent_coef = ent_coef,\n",
        "    verbose=1,\n",
        "    learning_rate = learning_rate,\n",
        "    seed=0,\n",
        "    tensorboard_log= tensorboard_log)\n",
        "\n",
        "model.learn(total_timesteps=total_timesteps, tb_log_name=tb_log_name)\n",
        "\n",
        "\n",
        "model.save(tensorboard_log+'model_'+tb_log_name)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "srSCLKsz94UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir \"$tensorboard_log\"\n"
      ],
      "metadata": {
        "id": "dxCvKqUP94UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "USBIplgW94UU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7D-UQ1-94UU"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'RCV1'\n",
        "\n",
        "\n",
        "qrels = \"data/qrels/rcv1_qrels_selected_45_0.2.txt\" # use the same qrel list as their rankings\n",
        "\n",
        "\n",
        "qrel_fname, query_rel_dic = load_rel_data(qrels)\n",
        "print(\"Number of topics:\", len(query_rel_dic))\n",
        "\n",
        "run = \"data/rankings/RCV1_selected_45_0.2_ranking.txt\"\n",
        "\n",
        "doc_rank_dic, rank_rel_dic = load_run_data(run)\n",
        "\n",
        "topics_list = make_topics_list(doc_rank_dic,1)  # sort topics by no docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zlf1Cnwu94UU"
      },
      "outputs": [],
      "source": [
        "# Instantiate the vec env\n",
        "\n",
        "#random topic selection for each env instance\n",
        "SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n",
        "\n",
        "vec_env = make_vec_env(TAREnv, n_envs=len(topics_list), env_kwargs=dict(target_recall=target_recall, topics_list = topics_list, topic_id=None, size=100, render_mode='human'))\n",
        "\n",
        "SELECTED_TOPICS = [] # reset before/after each call, keep track of all randomly selected topics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvyYvFdt94UU"
      },
      "outputs": [],
      "source": [
        "test_size = len(topics_list)\n",
        "vec_env_test = vec_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTai41gixQNr"
      },
      "source": [
        "#### TAR Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1gGWd5uxQNs"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.DataFrame()\n",
        "df_all_runs = pd.DataFrame()\n",
        "\n",
        "for run in range(total_runs):\n",
        "\n",
        "  # Test the trained agent\n",
        "  vec_env_test = vec_env\n",
        "  obs = vec_env_test.reset()\n",
        "  test_steps = 100\n",
        "\n",
        "\n",
        "  n_env = test_size\n",
        "  agent=0\n",
        "  target=0\n",
        "  agent_vector=[]\n",
        "  terminal_observation=[]\n",
        "\n",
        "  topics = []\n",
        "  recalls = []\n",
        "  costs=[]\n",
        "  e_costs = []\n",
        "  reliabilities = []\n",
        "  rewards = []\n",
        "  distances = []\n",
        "  differences = []\n",
        "  targets = []\n",
        "  run_cnts = []\n",
        "\n",
        "  for eID in range(test_size):\n",
        "\n",
        "    env = vec_env_test.envs[eID]\n",
        "    obs, info = env.reset()\n",
        "\n",
        "    for step in range(test_steps):\n",
        "      action, _ = model.predict(obs, deterministic=False) # predict all next steps\n",
        "      obs, reward, done, trun,info = env.step(action)\n",
        "\n",
        "\n",
        "      if done or trun:\n",
        "                  topic_id = info['topic_id']\n",
        "                  recall = info['recall']\n",
        "                  cost = info['cost']\n",
        "                  e_cost =  ((info['agent'] - info['target']) / (100-info['target']))\n",
        "                  distance = info['distance']\n",
        "\n",
        "                  agent = info['agent']\n",
        "                  target = info['target']\n",
        "                  agent_vector = info['agent_vector']\n",
        "                  terminal_observation = info['terminal_observation']\n",
        "\n",
        "                  difference = target_recall - recall\n",
        "\n",
        "                  reliability = 1 if recall >= target_recall else 0\n",
        "                  topics.append(topic_id)\n",
        "                  recalls.append(recall)\n",
        "                  costs.append(cost)\n",
        "                  e_costs.append(e_cost)\n",
        "                  reliabilities.append(reliability)\n",
        "                  rewards.append(reward)\n",
        "                  distances.append(distance)\n",
        "                  targets.append(target)\n",
        "                  run_cnts.append(run)\n",
        "                  differences.append(difference)\n",
        "\n",
        "                  df_tmp = pd.DataFrame( list(zip([dataset_name]*len(topics_list), topics, run_cnts, recalls, reliabilities, costs, e_costs, rewards, differences, distances, targets)),\n",
        "                  columns =['Dataset', 'Topic', 'Run', 'Recall', 'Reliability', 'Cost', 'e-Cost', 'Reward', 'Difference', 'Distance', 'Target'])\n",
        "\n",
        "                  df = pd.concat([df_tmp])\n",
        "\n",
        "                  df.groupby('Topic').mean()\n",
        "\n",
        "                  break\n",
        "\n",
        "  display(df)\n",
        "  df.groupby('Topic').mean()\n",
        "  df_all_runs = pd.concat([df_all_runs, df])\n",
        "\n",
        "df_all_runs['Model'] = model_name\n",
        "df_all_runs['Model_settings'] = tb_log_name\n",
        "df_all_runs['Target_Recall'] = target_recall\n",
        "df_all_runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClA0f1_MxQNs"
      },
      "outputs": [],
      "source": [
        "display(df_all_runs.groupby('Topic').mean())\n",
        "display(df_all_runs.groupby('Topic').std())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co2EKUmQxQNt"
      },
      "source": [
        "#### df_all_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0I7o7uQvxQNt"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_all_targets = pd.concat([df_all_targets, df_all_runs], ignore_index = True)\n",
        "\n",
        "\n",
        "display(df_all_targets)\n",
        "\n",
        "df_all_targets.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_all_targets.groupby(['Target_Recall','Dataset']).mean().round(3))\n",
        "display(df_all_targets.groupby(['Target_Recall','Dataset']).std().round(3))"
      ],
      "metadata": {
        "id": "NShx1II4xQNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### vis all datasets"
      ],
      "metadata": {
        "id": "nOkqi-tO94UW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_all_targets.groupby(['Target_Recall','Dataset']).mean().round(3))"
      ],
      "metadata": {
        "id": "QScQU4_494UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1iMzkiuvrJj"
      },
      "outputs": [],
      "source": [
        "(df_all_targets[['Target_Recall', 'Dataset', 'Recall' , 'Reliability', 'Cost', 'e-Cost']].groupby(['Target_Recall','Dataset']).mean().round(3)).to_latex()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "y00RfkU5gl3e",
        "pEsAU3iFchK7",
        "AjNCmL-zV_Xp",
        "HkcMI0zCV_Xr",
        "nEsMxqiPGBwD",
        "kfzuy5BvGBwJ"
      ],
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}